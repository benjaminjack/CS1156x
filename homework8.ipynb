{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning From the Data, Homework 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primal versus Dual Problem\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "Recall that $N$ is the size of the data set and $d$ is the dimensionality of the input space. The original formulation of the hard-margin SVM (minimize $\\frac{1}{2}\\mathbf{w}^T\\mathbf{w}$ subject to the inequality constraints), without going through the Lagrangian dual problem, is...\n",
    "\n",
    "#### Solution\n",
    "\n",
    "We are minimizing $\\mathbf{w}$ with constraints on $\\mathbf{w}$ and $b$. So, the number of variables will be the number of weights (length of $\\mathbf{w}$) plus one additional variable for $b$. The number of weights corresponds to the dimensionality $d$. Thus, the correct answer is a quadratic programming problem with $d+1$ variables.\n",
    "\n",
    "Answer: [**d**] a quadratic programming problem with $d+1$ variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with Soft Margins\n",
    "\n",
    "In the rest of the problems of this homework set, we apply soft-margin SVM to handwritten digits from the processed US Postal Service Code data set. Download the data (exracted features of intensity and symmetry) for training and testing:\n",
    "\n",
    "http://www.amlbook.com/data/zip/features.train\n",
    "\n",
    "http://www.amlbook.com/data/zip/features.test\n",
    "\n",
    "(the format of each row is: **digit intensity symmetry**). We will train two types of binary classifiers; one-versus-one (one digit is class +1 and another digit is class -1, with the rest of the digits disregarded), and one-versus-all (one digit is class +1 and the rest of the digits are class -1).\n",
    "\n",
    "Implement SVM with soft margin on the above zip-code data set by solving:\n",
    "\n",
    "$\\min_\\alpha \\frac{1}{2}\\sum_{n=1}^N\\sum_{m=1}^N\\alpha_{n}\\alpha_{m}y_{n}y_{m}K(\\mathbf{x}_n\\mathbf{x}_m)-\\sum_{n=1}^N\\alpha_n$\n",
    "\n",
    "$\\text{s.t.} \\sum_{n=1}^{N}y_n\\alpha_n = 0$\n",
    "\n",
    "$0 \\leq \\alpha_n \\leq C \\quad n=1,...,N$\n",
    "\n",
    "When evaluating $E_\\text{in}$ and $E_\\text{out}$ of the resulting classifier, use binary classification error.\n",
    "\n",
    "## Polynomial Kernels\n",
    "\n",
    "Consdier the polynomial kernel $K(\\mathbf{x}_n, \\mathbf{x}_m) = (1 + \\mathbf{x}_n^T\\mathbf{x}_m)^Q$, where $Q$ is the degree of the polynomial.\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "With $C=0.01$ and $Q=2$, which of the following classifiers has the **highest** $E_\\text{in}$?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digit  E_in       SVs       \n",
      "0      0.10588397 2179      \n",
      "1      0.01440132 386       \n",
      "2      0.10026060 1970      \n",
      "3      0.09024825 1950      \n",
      "4      0.08942532 1856      \n",
      "5      0.07625840 1585      \n",
      "6      0.09107118 1893      \n",
      "7      0.08846523 1704      \n",
      "8      0.07433823 1776      \n",
      "9      0.08832808 1978      \n"
     ]
    }
   ],
   "source": [
    "train = pd.read_table(\"http://www.amlbook.com/data/zip/features.train\", \n",
    "                      delim_whitespace=True, header=None).values\n",
    "\n",
    "def one_versus(d, num, num2=None):\n",
    "    d = d.copy()\n",
    "    if num2 is not None:\n",
    "        d = d[(d[:,0] == num) | (d[:,0] == num2),:]\n",
    "    d[d[:,0] == num, 0] = -1\n",
    "    d[d[:,0] != -1, 0] = -2\n",
    "    d[d[:,0] == -1, 0] = 1\n",
    "    d[d[:,0] == -2, 0] = 0\n",
    "    return d[:,1:], d[:,0]\n",
    "\n",
    "print(\"{:<6} {:<10} {:<10}\".format(\"Digit\", \"E_in\", \"SVs\"))\n",
    "for i in range(10):\n",
    "    X, Y = one_versus(train, i)\n",
    "    clf = SVC(kernel='poly', degree=2, gamma=1.0, coef0=1.0, C=0.01)\n",
    "    clf.fit(X, Y)\n",
    "    print(\"{:<6} {:<10.8f} {:<10}\"\n",
    "          .format(i, 1 - clf.score(X, Y), np.sum(clf.n_support_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**a**] 0 versus all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "With $C=0.01$ and $Q=2$, which of the following classifiers has the lowest $E_\\text{in}$?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "Answer: [**a**] 1 versus all\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probem 4\n",
    "\n",
    "Comparing the two selected classifiers from Problems 2 and 3, which of the following values is the closest to the difference between the number of support vectors of these two classifiers?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1793"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2179-386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**c**] 1800"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 5\n",
    "\n",
    "Consider the 1 versus 5 clssifier with $Q=2$ and $C \\in \\{0.001, 0.01, 0.1, 1\\}$. Which of the following statements is correct? Going up or down means strictly so.\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C      E_in       E_out      SVs       \n",
      "0.001  0.00448430 0.01650943 76        \n",
      "0.01   0.00448430 0.01886792 34        \n",
      "0.1    0.00448430 0.01886792 24        \n",
      "1.0    0.00320307 0.01886792 24        \n"
     ]
    }
   ],
   "source": [
    "test = pd.read_table(\"http://www.amlbook.com/data/zip/features.test\", \n",
    "                      delim_whitespace=True, header=None).values\n",
    "\n",
    "X, Y = one_versus(train, 1, 5)\n",
    "X_test, Y_test = one_versus(test, 1, 5)\n",
    "\n",
    "print(\"{:<6} {:<10} {:<10} {:<10}\"\n",
    "      .format(\"C\", \"E_in\", \"E_out\", \"SVs\"))\n",
    "for i in [0.001, 0.01, 0.1, 1.0]:\n",
    "    clf = SVC(kernel='poly', degree=2, gamma=1.0, coef0=1.0, C=i)\n",
    "    clf.fit(X, Y)\n",
    "    e_in = 1 - clf.score(X, Y)\n",
    "    e_out = 1 - clf.score(X_test, Y_test)\n",
    "    print(\"{:<6} {:<10.8f} {:<10.8f} {:<10}\"\n",
    "          .format(i, e_in, e_out, np.sum(clf.n_support_)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**d**] Maximum $C$ achieves the lowest $E_\\text{in}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "In the 1 versus 5 classifier, comparing $Q = 2$ with $Q = 5$, which of the following statements is correct?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q  C      E_in       E_out      SVs       \n",
      "2  0.0001 0.00896861 0.01650943 236       \n",
      "5  0.0001 0.00448430 0.01886792 26        \n",
      "2  0.001  0.00448430 0.01650943 76        \n",
      "5  0.001  0.00448430 0.02122642 25        \n",
      "2  0.01   0.00448430 0.01886792 34        \n",
      "5  0.01   0.00384369 0.02122642 23        \n",
      "2  1.0    0.00320307 0.01886792 24        \n",
      "5  1.0    0.00320307 0.02122642 21        \n"
     ]
    }
   ],
   "source": [
    "print(\"{:<2} {:<6} {:<10} {:<10} {:<10}\"\n",
    "      .format(\"Q\", \"C\", \"E_in\", \"E_out\", \"SVs\"))\n",
    "for i in [0.0001, 0.001, 0.01, 1.0]:\n",
    "    for q in [2, 5]:\n",
    "        clf = SVC(kernel='poly', degree=q, gamma=1.0, coef0=1.0, C=i)\n",
    "        clf.fit(X, Y)\n",
    "        e_in = 1 - clf.score(X, Y)\n",
    "        e_out = 1 - clf.score(X_test, Y_test)\n",
    "        print(\"{:<2} {:<6} {:<10.8f} {:<10.8f} {:<10}\"\n",
    "              .format(q, i, e_in, e_out, np.sum(clf.n_support_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**b**] When $C=0.001$, the number of support vectors is lower than $Q=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation\n",
    "\n",
    "In the next two problems, we will experiment with 10-fold cross validation for the polynomial kernel. Because $E_\\text{cv}$ is a random variable that depends on the random partition of the data, we will try 100 runs with different partitions and base our answer on how many runs lead to a particular choice.\n",
    "\n",
    "### Problem 7\n",
    "\n",
    "Consdier the 1 versus 5 classifier with $Q=2$. We use $E_\\text{cv}$ to select $C \\in \\{0.0001, 0.001, 0.01, 0.1, 1\\}$. If there is a tie in $E_\\text{cv}$, select the smaller $C$. Within the 100 rnadom runs, which of the following statements is correct?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': 0, '1': 30, '2': 25, '3': 24, '4': 21} 0.00474077249714\n"
     ]
    }
   ],
   "source": [
    "def run_experiment():\n",
    "    k_fold = KFold(n_splits=10, shuffle=True)\n",
    "    X, Y = one_versus(train, 1, 5)\n",
    "    E_cv = np.zeros(5)\n",
    "    for idx, c in enumerate([0.0001, 0.001, 0.01, 0.1, 1]):\n",
    "        clf = SVC(kernel='poly', degree=2, gamma=1.0, coef0=1.0, C=c)\n",
    "        accuracy = [clf.fit(X[train_idx], Y[train_idx])\n",
    "                        .score(X[test_idx], Y[test_idx])\n",
    "                    for train_idx, test_idx \n",
    "                    in k_fold.split(Y)]\n",
    "        E_cv[idx] = np.mean(1 - np.array(accuracy))\n",
    "    return E_cv, np.min(np.where(E_cv == np.min(E_cv)))\n",
    "\n",
    "counts = {'0':0, '1':0, '2':0, '3':0, '4':0}\n",
    "\n",
    "total_E = 0.0\n",
    "\n",
    "for i in range(100):\n",
    "    E_cv, idx = run_experiment()\n",
    "    total_E += E_cv[1]\n",
    "    counts[str(idx)] += 1\n",
    "\n",
    "print(counts, total_E/100)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**b**] $C = 0.001$ is selected most often."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8\n",
    "\n",
    "Again, consider the 1 versus 5 classifier with $Q=2$. For the winning selection in the previous problem, the average value of $E_\\text{cv}$ over the 100 runs is closest to\n",
    "\n",
    "#### Solution\n",
    "\n",
    "See code and output from Problem 7.\n",
    "\n",
    "Answer: [**c**] 0.005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBF Kernel\n",
    "\n",
    "Consider the radial basis function (RBF) kernel $K(\\mathbf{x}_n, \\mathbf{x}_m) = \\exp (-||\\mathbf{x}_n - \\mathbf{x}_m||^2)$ in the soft-margin SVM approach. Focus on the 1 versus 5 classifier.\n",
    "\n",
    "### Problem 9\n",
    "\n",
    "Which of the following values of $C$ results in the lowest $E_\\text{in}$?\n",
    "\n",
    "#### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C      E_in       E_out      SVs       \n",
      "1E-02  0.00384369 0.02358491 406       \n",
      "1E+00  0.00448430 0.02122642 31        \n",
      "1E+02  0.00320307 0.01886792 22        \n",
      "1E+04  0.00256246 0.02358491 19        \n",
      "1E+06  0.00064061 0.02358491 17        \n"
     ]
    }
   ],
   "source": [
    "X, Y = one_versus(train, 1, 5)\n",
    "X_test, Y_test = one_versus(test, 1, 5)\n",
    "\n",
    "print(\"{:<6} {:<10} {:<10} {:<10}\"\n",
    "      .format(\"C\", \"E_in\", \"E_out\", \"SVs\"))\n",
    "for i in [0.01, 1, 100, 1e4, 1e6]:\n",
    "    clf = SVC(kernel='rbf', gamma=1.0, C=i)\n",
    "    clf.fit(X, Y)\n",
    "    e_in = 1 - clf.score(X, Y)\n",
    "    e_out = 1 - clf.score(X_test, Y_test)\n",
    "    print(\"{:<6.0E} {:<10.8f} {:<10.8f} {:<10}\"\n",
    "          .format(i, e_in, e_out, np.sum(clf.n_support_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**e**] $10^6$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10\n",
    "\n",
    "Which of the following values of C results in the lowest $E_\\text{out}$?\n",
    "\n",
    "#### Solution\n",
    "\n",
    "See code and output from Problem 9.\n",
    "\n",
    "Answer: [**c**] $C=100$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
