{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning from the Data, Homework 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Error\n",
    "\n",
    "Consider a noisy target $y = \\mathbf{w}^{∗T} \\mathbf{x} + \\epsilon$, where $\\mathbf{x} \\in \\mathbb{R}^d$ (with the added coordinate $x_0 = 1$), $y \\in \\mathbb{R}$, $\\mathbf{w}^∗$ is an unknown vector, and $\\epsilon$ is a noise term with zero mean and $\\sigma^2$ variance. Assume $\\epsilon$ is independent of $\\mathbf{x}$ and of all other $\\epsilon$’s. If linear regression is carried out using a training data set $\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), . . . , (\\mathbf{x}_N, y_N )\\}$, and outputs the parameter vector $\\mathbf{w}_\\text{lin}$, it can be shown that the expected in-sample error $E_\\text{in}$ with respect to $\\mathcal{D}$ is given by:\n",
    "\n",
    "$\\mathbb{E}_\\mathcal{D}[E_\\text{in}(\\mathbf{w}_\\text{lin})] = \\sigma^2 \\left(1 - \\frac{d + 1}{N}\\right)$\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "For $sigma = 0.1$ and $d = 8$, which among the following choices is the smallest number of examples $N$ that will result in an expected $E_\\text{in}$ greater than 0.008?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: 0.001\n",
      "25: 0.006400000000000001\n",
      "100: 0.009100000000000002\n",
      "500: 0.009820000000000002\n",
      "1000: 0.009910000000000002\n"
     ]
    }
   ],
   "source": [
    "def e_in(N, sigma=0.1, d=8.0):\n",
    "    return (sigma**2)*(1-((d + 1)/N))\n",
    "    \n",
    "for N in [10, 25, 100, 500, 1000]:\n",
    "    print(str(N) + \": \" + str(e_in(N)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**c**] 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear Transforms\n",
    "\n",
    "In the linear classification, consider the feature transform $\\Phi : \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2$ (plus the added zeroth coordinate) given by:\n",
    "\n",
    "$\\Phi(1, x_1, x_2) = (1, x_1^2, x_2^2)$\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Which of the following sets of constraints on the weights in the $\\mathcal{Z}$ space could correspond to the hyperbolic decision boundary in $\\mathcal{X}$ depicted in the figure?\n",
    "\n",
    "You may assume that $\\tilde{w}_0$ can be selected to achieve the desired boundary.\n",
    "\n",
    "$w_0*1 + w_1*x_1^2 + w_2*x_2^2 = 0$\n",
    "\n",
    "$x_2 = \\sqrt{\\frac{-w_0 - w_1*x_1^2}{w_2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel/__main__.py:6: RuntimeWarning: invalid value encountered in sqrt\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAD8CAYAAABzTgP2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUdo9VWObAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10dbbdf98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15\n",
      "0.15\n",
      "-0.9750000000000001\n",
      "-0.9750000000000001\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def hyperbolic(w0, w1, w2, x1):\n",
    "    x2 = np.sqrt((-w0 - w1*x1**2)/w2)\n",
    "    return x2, -x2 \n",
    "\n",
    "x1 = np.linspace(-1, 1)\n",
    "\n",
    "# Pick values of w0, w1, w2, until you get a shape that approximates\n",
    "# boundaries that we are given.\n",
    "x2, x2_neg = hyperbolic(0.01, -1, 1, x1)\n",
    "\n",
    "plt.plot(x1, x2, \n",
    "         x1, x2_neg)\n",
    "plt.show()\n",
    "\n",
    "def evaluate(w0, w1, w2, x1, x2):\n",
    "    return w0*1 + w1*x1**2 + w2*x2**2\n",
    "\n",
    "# Evaluate points on either side of boundary to verify that the inner\n",
    "# region is +1 and left and right sides are -1.\n",
    "print(evaluate(-0.10, -2, 1, 0, 0.5))\n",
    "print(evaluate(-0.10, -2, 1, 0, -0.5))\n",
    "print(evaluate(-0.10, -2, 1, 0.75, 0.5))\n",
    "print(evaluate(-0.10, -2, 1, -0.75, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**d**] $\\tilde{w_1} < 0, \\tilde{w_2} > 0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3\n",
    "\n",
    "Now, consider the 4th order polynomial transform from the input space $\\mathbb{R}^2$:\n",
    "\n",
    "$\\Phi_4 : \\mathbb{x} \\rightarrow (1, x_1, x_2, x_1^2, x_2^2, x_1^3, x_1^2x_2, x_1, x_2^2, x_2^3, x_1^4, x_1^3x_2, x_1^2x_2^2, x_1x_2^3, x_2^4)$\n",
    "\n",
    "What is the smallest value among the following choices that is *not* smaller than the VC dimension of a linear model in this transformed space?\n",
    "\n",
    "Since there are 15 parameters in the transformed space, then $d_\\text{vc} \\leq 15$.\n",
    "\n",
    "Answer: [**c**] 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Consider the nonlinear error surface $E(u,v) = (ue^v - 2ve^{-u})^2$. We start at the point $(u,v) = (1,1)$ and minimize this error using gradient descent in the $uv$ space. Use $\\eta = 0.1$ (learning rate, not step size).\n",
    "\n",
    "### Problem 4\n",
    "\n",
    "What is the partial derivative of $E(u,v)$ with respect to $u$, i.e., $\\frac{\\partial E}{\\partial u}$?\n",
    "\n",
    "By the chain rule, $\\frac{\\partial E}{\\partial u} = 2(ue^v - 2ve^{-u})(e^v + 2ve^{-u})$.\n",
    "\n",
    "Answer: [**e**] $2(e^v + 2ve^{-u})(ue^v - 2ve^{-u})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5\n",
    "\n",
    "How many iterations (among the given choices) does it take for the error $E(u,v)$ to fall below $10^{-14}$ for the first time? In your programs, make sure to use double precision to get the needed accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting error: 3.93039723188\n",
      "iterations: 10\n",
      "error: 1.20868339442e-15\n"
     ]
    }
   ],
   "source": [
    "def E_in(u, v):\n",
    "    return (u*np.exp(v) - 2*v*np.exp(-u))**2\n",
    "\n",
    "def du(u, v):\n",
    "    return 2*(np.exp(v) + 2*v*np.exp(-u))*(u*np.exp(v) - 2*v*np.exp(-u))\n",
    "\n",
    "def dv(u, v):\n",
    "    return 2*(u*np.exp(v) - 2*np.exp(-u))*(u*np.exp(v) - 2*np.exp(-u)*v)\n",
    "\n",
    "n = 0  # iterations\n",
    "u = 1.0\n",
    "v = 1.0\n",
    "eta = 0.1\n",
    "error = E_in(u, v)\n",
    "print(\"starting error:\", error)\n",
    "\n",
    "while error > 10e-14:\n",
    "    delta_u = eta*du(u, v)\n",
    "    delta_v = eta*dv(u, v)\n",
    "    u -= delta_u\n",
    "    v -= delta_v\n",
    "    error = E_in(u, v)\n",
    "    n += 1\n",
    "\n",
    "print(\"iterations:\", n)\n",
    "print(\"error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**d**] 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6\n",
    "\n",
    "After running enough iterations such that the error has just dropped below $10^{-14}$, what are the closest values (in Euclidean distance) among the following choices to the final $(u, v)$ you go in Problem 5?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final (u, v): 0.0447362903978 0.0239587140991\n"
     ]
    }
   ],
   "source": [
    "print(\"final (u, v):\", u, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**e**] (0.045, 0.024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7\n",
    "\n",
    "Now, we will compare the performance of \"coordinate descent.\" In each iteration, we have two steps along the 2 coordinates. Step 1 is to move only along the $u$ coordinate to reduce the error (assume first-order approximation holds like in gradient descent), and step 2 is to reevaluate and move only along the $v$ coordinate to reduce the error (again, assume first-order approximation holds). Use the same learning rate of $\\eta = 0.1$ as we did in gradient descent. What will the error $E(u,v$ be closest to after 15 full iterations (30 steps)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting error: 3.93039723188\n",
      "error: 0.139813791996\n"
     ]
    }
   ],
   "source": [
    "u = 1.0\n",
    "v = 1.0\n",
    "eta = 0.1\n",
    "error = E_in(u, v)\n",
    "print(\"starting error:\", error)\n",
    "\n",
    "for i in range(15):\n",
    "    # Step 1\n",
    "    delta_u = eta*du(u, v)\n",
    "    u -= delta_u\n",
    "    # Step 2\n",
    "    delta_v = eta*dv(u, v)\n",
    "    v -= delta_v\n",
    "    error = E_in(u, v)\n",
    "\n",
    "print(\"error:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**a**] $10^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "In this problem you will create your own target function $f$ (probability in this case) and data set $\\mathcal{D}$ to see how Logistic Regression works. For simplicity, we will take $f$ to be a 0/1 probability so $y$ is a deterministic function of $\\mathbf{x}$.\n",
    "\n",
    "Take $d = 2$ so you can visualize the problem, and let $\\mathcal{X} = [-1, 1]\\times[-1,1]$ with uniform probability of picking each $\\mathbf{x} \\in \\mathcal{X}$. Choose a line in the plane as the boundary between $f(\\mathbf{x}) = 1$ (where $y$ has to be +1) and $f(\\mathbf{x}) = 0$ (where $y$ has to be -1) by taking two random, uniformly distributed points from $\\mathcal{X}$ and taking the line passing through them as the boundary between $y = \\pm1$. Pick $N = 100$ traning points at random from $\\mathcal{X}$, and evaluate the outputs $y_n$ for each of these points $\\mathbf{x}_n$.\n",
    "\n",
    "Run Logistic Regression with Stochastic Gradient Descent to find $g$, and estimate $E_\\text{out}$ (the **cross entropy** error) by generating a sufficiently large, separate set of points to evaluate the error. Repeat the experiment for 100 runs with different targets and take the average. Initialize the weight vector of Logistic Regression to all zeros in each run. Stop the algorithm when $\\| \\mathbf{w}^{(t-1)} - \\mathbf{w}^{(t)}\\| < 0.01$, where $\\mathbf{w}^{(t)}$ denotes the weight vector at the end of epoch $t$. An epoch is a full pass through the $N$ data points (use a random permutation of 1, 2, ..., $N$ to present the data points to the algorithm within each epoch, and use different permutations for different epochs). Use a learning rate of 0.01.\n",
    "\n",
    "### Problem 8\n",
    "\n",
    "Which of the following is closest to $E_\\text{out}$ for $N = 100$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean out of sample error: [[ 0.10123029]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1)\n",
    "def generate_line():\n",
    "    x_rand = np.random.uniform(-1, 1, size=(2,2))\n",
    "    slope = (x_rand[0,1] - x_rand[0,0])/(x_rand[1,1] - x_rand[1,0])\n",
    "    intercept = x_rand[0,1] - slope*x_rand[0,0]\n",
    "    W_true = np.array([[intercept], [slope], [-1]])\n",
    "    return W_true\n",
    "\n",
    "def generate_points(W_true, N=100):\n",
    "    ones = np.ones((1, N))\n",
    "    X = np.vstack((ones, np.random.uniform(-1,1, size=(2,N))))\n",
    "    Y = np.sign(np.dot(W_true.T, X))\n",
    "    return X, Y\n",
    "\n",
    "def shuffle_examples(X, Y):\n",
    "    X_Y = np.vstack((X, Y)).T\n",
    "    np.random.shuffle(X_Y)\n",
    "    X_Y = X_Y.T\n",
    "    X = X_Y[0:3, :]\n",
    "    Y = X_Y[[3], :]\n",
    "    return X, Y\n",
    "\n",
    "def estimate_e_out(W_true, W):\n",
    "    X, Y = generate_points(W_true, N=1000)\n",
    "    error = 0\n",
    "    for i in range(X.shape[1]):\n",
    "        x = X[:, [i]]\n",
    "        y = Y[:, i]\n",
    "        error += np.log(1 + np.exp(-y*np.dot(W.T,x)))/1000\n",
    "    return error\n",
    "\n",
    "def grad_descent(W_true, X, Y, eta = 0.01, max_epochs = 10000):\n",
    "    W = np.zeros((3,1))\n",
    "    m_examples = X.shape[1]\n",
    "    for n in range(max_epochs):\n",
    "        cost = 0\n",
    "        W_old = np.copy(W)\n",
    "        X, Y = shuffle_examples(X, Y)\n",
    "        for i in range(m_examples):\n",
    "            x = X[:, [i]]\n",
    "            y = Y[:, i]\n",
    "            grad = -(y*x)/(1 + np.exp(y*np.dot(W.T,x)))\n",
    "            W = W - eta*grad\n",
    "            cost += np.log(1 + np.exp(-y*np.dot(W.T,x)))/m_examples\n",
    "        if np.linalg.norm(W_old-W) < 0.01:\n",
    "            return cost, n, W, estimate_e_out(W_true, W)\n",
    "\n",
    "W_true = generate_line()\n",
    "X, Y = generate_points(W_true)\n",
    "\n",
    "mean_epochs = 0\n",
    "mean_e_out = 0\n",
    "for i in range(100):\n",
    "    W_true = generate_line()\n",
    "    X, Y = generate_points(W_true)\n",
    "    cost, epochs, W, e_out = grad_descent(W_true, X, Y)\n",
    "    mean_epochs += epochs/100\n",
    "    mean_e_out += e_out/100\n",
    "\n",
    "print(\"Mean out of sample error:\", mean_e_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Answer: [**d**] 0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Problem 9\n",
    "\n",
    "How many epochs does it take on average for Logistic Regression to converge for $N = 100$ using the above initialization and termination rules and the specified learning rate? Pick the value that is closest to your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean epochs to convergence: 324.86999999999995\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean epochs to convergence:\", mean_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**a**] 350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLA as SGD\n",
    "\n",
    "### Problem 10\n",
    "\n",
    "The Perceptron Learning Algorithm can be implemented as SGD using which of the following error functions $e_n(\\mathbf{w})$? Ignore the points $\\mathbf{w}$ at which $e_n(\\mathbf{w})$ is not twice differentiable.\n",
    "\n",
    "The error should be 0 if all points are classified correctly, and positive otherwise. The function $e_n(\\mathbf{w}) = −\\min(0,y_n\\mathbf{w}^\\text{T}x_n)$ is the only error function that meets these criteria.\n",
    "\n",
    "Answer: [**e**] $e_n(\\mathbf{w}) = −\\min(0,y_n\\mathbf{w}^\\text{T}x_n)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
