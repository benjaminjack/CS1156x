{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning From the Data, Homework 6\n",
    "\n",
    "## Overfitting and deterministic noise\n",
    "\n",
    "### Problem 1\n",
    "\n",
    "Deterministic noise depends on $\\mathcal{H}$, as some models approximate $f$ better than others. Assume that $\\mathcal{H'} \\subset \\mathcal{H}$ and that $f$ is fixed. **In general** (but not necessarily in all cases), if we use $\\mathcal{H'}$ instead of $\\mathcal{H}$, how does deterministic noise behave?\n",
    "\n",
    "Assuming that there are fewer hypotheses in set $\\mathcal{H'}$ than $\\mathcal{H}$, then $\\mathcal{H'}$ is less likely to approximate $f$ as well as $\\mathcal{H}$, therefore deterministic noise will increase.\n",
    "\n",
    "Answer: [**b**] In general, deterministic noise will increase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization with Weight Decay\n",
    "\n",
    "In the following problems use the data provided in the files:\n",
    "\n",
    "http://work.caltech.edu/data/in.dta\n",
    "\n",
    "http://work.caltech.edu/data/out.dta\n",
    "\n",
    "as a training and test set respectively. Each line of the files corresponds to a two-dimensional input $\\mathbf{x} = (x_1, x_2)$, so that $\\mathcal{X} = \\mathbb{R}^2$, followed by the corresponding label from $\\mathcal{Y} = \\{-1, 1\\}$. We are going to apply Linear Regression with a non-linear transformation for classification. The nonlinear transformation is given by \n",
    "\n",
    "$\\Phi(x_1, x_2) = (1, x_1, x_2, x_1^2, x_2^2, x_1x_2, \\left|x_1 - x_2\\right|, \\left|x_1 + x_2\\right|)$.\n",
    "\n",
    "Recall that the classification error is defined as the fraction of misclassified points.\n",
    "\n",
    "### Problem 2\n",
    "\n",
    "Run Linear Regression on the training set after performing the non-linear transformation. What values are closest (in Euclidean distance) to the in-sample and out-of-sample classifcation errors, respectively?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.028571428571428571, 0.084000000000000005)\n",
      "(0.028571428571428571, 0.080000000000000002)\n",
      "(0.37142857142857144, 0.436)\n",
      "2 : (0.20000000000000001, 0.22800000000000001)\n",
      "1 : (0.057142857142857141, 0.124)\n",
      "0 : (0.0, 0.091999999999999998)\n",
      "-1 : (0.028571428571428571, 0.056000000000000001)\n",
      "-2 : (0.028571428571428571, 0.084000000000000005)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def linear_regression(data, W=None, lamd=0):\n",
    "    # Extract and transform data\n",
    "    X1 = data[:,0]\n",
    "    X2 = data[:,1]\n",
    "    Y = data[:,[2]]\n",
    "    \n",
    "    Z = np.column_stack((np.ones(X1.shape), \n",
    "                         X1, \n",
    "                         X2, \n",
    "                         X1**2, \n",
    "                         X2**2,\n",
    "                         X1*X2,\n",
    "                         np.abs(X1 - X2),\n",
    "                         np.abs(X1 + X2)))\n",
    "    # Compute W if not given\n",
    "    if W is None:\n",
    "        Z_square = np.dot(Z.T, Z)\n",
    "        W = np.dot(np.linalg.inv(Z_square + np.identity(Z_square.shape[0])*lamd), np.dot(Z.T, Y))\n",
    "    \n",
    "    Y_hat = np.sign(np.dot(W.T, Z.T))\n",
    "    error = np.sum(Y_hat.T != Y)/Y.shape[0]\n",
    "    return error, W\n",
    "\n",
    "def run_regression(train, test, lamd=0):\n",
    "    # Compute E_in and E_out\n",
    "    E_in, W_lin = linear_regression(train, lamd=lamd)\n",
    "    E_out, _ = linear_regression(test, W=W_lin)\n",
    "    return E_in, E_out\n",
    "\n",
    "# Read in data\n",
    "train = pd.read_table(\"http://work.caltech.edu/data/in.dta\", delim_whitespace=True, header=None).values\n",
    "test = pd.read_table(\"http://work.caltech.edu/data/out.dta\", delim_whitespace=True, header=None).values\n",
    "\n",
    "# Compute E_in and E_out\n",
    "print(run_regression(train, test))\n",
    "\n",
    "print(run_regression(train, test, lamd=10**(-3)))\n",
    "print(run_regression(train, test, lamd=10**(3)))\n",
    "\n",
    "for i in [2, 1, 0, -1, -2]:\n",
    "    print(i, \":\", run_regression(train, test, lamd=10**(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: [**a**] 0.03, 0.08"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
